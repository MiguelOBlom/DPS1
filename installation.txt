Deployment of systems
Installing Apache Spark 3.1.2 with Hadoop 2.7:
https://phoenixnap.com/kb/install-spark-on-ubuntu
Installing Turi 6.4.1:
https://github.com/apple/turicreate/#documentation
Apache Giraph 1.3.0: (Not going for 1.4.0 since it is a snapshot)
https://giraph.apache.org/

Getting the data:
- Twitter: Currently using https://snap.stanford.edu/data/twitter-2010.html
unzip the twitter-2010.txt.gz file and took first 10000 hits (for testing on low memory machine) -> twitter-2010@10000.txt
unzip the twitter-2010.ids.csv.gz and took out the header  -> twitter-2010.ids.txt
However, the data should really be from LAW, https://law.di.unimi.it/webdata/twitter-2010/
To use twitter-2010.graph -> decompress using WebGraph for Hadoop
To use twitter-2010.ids.gz -> instead of splitting on ',', split on ' '

+ Added proof of concept in graphx-pagerank.scala

Using the data
WEBGRAPH ------
C++ WebGraph?
Install boost
Also error while making

Doesnt work either
#Use WebGraph https://github.com/vigna/webgraph
#install ant and ivy



OUTDATED:
#Use WebGraph: https://webgraph.di.unimi.it/#Hadoop
#Install Maven
#sudo apt-get install maven
#While installing WebGraph set the JAVA_HOME variable to your JDK location #(readlink -f $(which java))
#export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/bin/java


--- Webgraph + GraphX:

import org.apache.spark.graphx._
import de.l3s.mapreduce.webgraph.io._

WebGraphInputFormat.setBasename(sc.hadoopConfiguration, "/hdfs/path/to/webgraph/basename")
WebGraphInputFormat.setNumberOfSplits(sc.hadoopConfiguration, 100)

val rdd = sc.newAPIHadoopRDD(sc.hadoopConfiguration, classOf[WebGraphInputFormat], classOf[IntWritable], classOf[IntArrayWritable])
val edges = rdd.flatMap{case (id, out) => out.values.map(outId => (id.get.toLong, outId.toLong))}
val graph = Graph.fromEdgeTuples(edges, true)

--- Then the GraphX implementation for PageRank: https://spark.apache.org/docs/latest/graphx-programming-guide.html#pagerank

// Run PageRank
val ranks = graph.pageRank(0.0001).vertices
// Join the ranks with the usernames
val users = sc.textFile("data/graphx/users.txt").map { line =>
  val fields = line.split(",")
  (fields(0).toLong, fields(1))
}
val ranksByUsername = users.join(ranks).map {
  case (id, (username, rank)) => (username, rank)
}
// Print the result
println(ranksByUsername.collect().mkString("\n"))

--- And Connected Components: https://spark.apache.org/docs/latest/graphx-programming-guide.html#connected-components

// Find the connected components
val cc = graph.connectedComponents().vertices
// Join the connected components with the usernames
val users = sc.textFile("data/graphx/users.txt").map { line =>
  val fields = line.split(",")
  (fields(0).toLong, fields(1))
}
val ccByUsername = users.join(cc).map {
  case (id, (username, cc)) => (username, cc)
}
// Print the result
println(ccByUsername.collect().mkString("\n"))









OLD! The following instructions will not be used, but this is a note
We have tried updating the broken URLs in the installation files of GraphX and GraphLab, however we could not get the installations to work afterwards. Therefore, we will be using the newest versions of all systems.



================
GRAPHX
================

Download and extract the following file
http://d3kbcqa49mib13.cloudfront.net/spark-0.9.1.tgz

In project/build.properties, change the sbt.version to 0.13.18 
sbt.version=0.13.18
(DO NOT ADD ANY WHITESPACE AFTER THE VERSION SPECIFICATION)

Download and extract sbt from the following url
https://github.com/sbt/sbt/releases/download/v0.13.18/sbt-0.13.18.tgz
Copy sbt-launch.jar from sbt/bin/sbt-launch.jar from this extracted folder to sbt/sbt-launch.jar in the spark folder
Rename sbt-launch.jar to sbt-launch-0.13.18.jar

Run the following command from the spark root folder
sbt/sbt assembly

We will see some warnings, after a short while we will see downloads starting.

In project/plugins.sbt, add the following on line 4:
resolvers += "Instantor Repository" at "https://www.instantor.com/nexus/content/groups/public/"
Also change line 18 to
addSbtPlugin("net.virtual-void" % "sbt-dependency-graph" % "0.7.4")

================
GRAPHLAB
================
Prerequisites:
Make sure cmake, ant are installed:

sudo apt-get install cmake
sudo apt-get install ant

For Zookeeper:
sudo apt-get install python-setuptools
sudo apt-get install libcppunit-dev

Change lines 358 and 359 in /CMakeList.txt in the graphlab folder to:
    URL https://downloads.sourceforge.net/project/levent/release-2.0.22-stable/libevent-2.0.22-stable.tar.gz
    URL_MD5 c4c56f986aa985677ca1db89630a2e11
(i.e. Update the download link for libevent) 

Do the same for gperftools
    URL https://downloads.sourceforge.net/project/gperftools.mirror/gperftools-2.9/gperftools-2.9.tar.gz
    URL_MD5 df0a09b04747272a7faaa7cfd1eb0d73

And for zookeeper
  URL https://mirror.softaculous.com/apache/zookeeper/zookeeper-3.5.9/apache-zookeeper-3.5.9.tar.gz
  URL_MD5 ee376ba90fde472cb14e6c58b20877a7

(MD5 generated using gpg --print-md MD5 apache-zookeeper-3.5.9.tar.gz)


zookeeper
https://archive.apache.org/dist/zookeeper/zookeeper-3.5.1-alpha/zookeeper-3.5.1-alpha.tar.gz
d85f9751724d3f20f792803b61c4db24

bzip2
https://downloads.sourceforge.net/project/bzip2/bzip2-1.0.6.tar.gz
00b516f4704d4a7cb50a1d97e6e8e15b

boost
https://downloads.sourceforge.net/project/boost/boost/1.53.0/boost_1_53_0.tar.gz
57a9e2047c0f511c4dfcf00eb5eb2fbb

gperftools
https://src.fedoraproject.org/lookaside/pkgs/gperftools/gperftools-2.0.tar.gz/13f6e8961bc6a26749783137995786b6/gperftools-2.0.tar.gz
13f6e8961bc6a26749783137995786b6

libevent
https://github.com/downloads/libevent/libevent/libevent-2.0.18-stable.tar.gz
aa1ce9bc0dee7b8084f6855765f2c86a

~/Desktop/graphlab/deps/zookeeper/src/zookeeper$ nano src/c/Makefile.in
remove -Werror in Makefile.am, Makefile.in

eigen
https://gitlab.com/libeigen/eigen/-/archive/3.1.2/eigen-3.1.2.tar.bz2
b265ae4786060a610a8921bc939770bb

Installing GraphLab:

git clone https://github.com/graphlab-code/graphlab.git
cd graphlab
./configure
cd release/toolkits/graph_analytics
make -j4